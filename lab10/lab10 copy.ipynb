{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, curr_player):\n",
    "        self.board_rows = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # rows\n",
    "        self.board_cols = [[1, 4, 7], [2, 5, 8], [3, 6, 9]]  # columns\n",
    "        self.board_diag = [[1, 5, 9], [3, 5, 7]]             # diagonals\n",
    "        self.x_positions = set()\n",
    "        self.o_positions = set()\n",
    "        self.free_positions = set(range(1, 10)) # at the beginning they are all free\n",
    "        self.current_player = curr_player\n",
    "\n",
    "    def make_move(self, position):\n",
    "        # Adds a position w.r.t the player taken positions\n",
    "        self.x_positions.add(position) if self.current_player == 'X' else self.o_positions.add(position)\n",
    "\n",
    "\n",
    "\n",
    "    ### ALL THESE FUNCTIONS ARE CALLED ONCE AFTER THE MOVE HAS BEEN DONE ###\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, and diagonals for a win\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in self.x_positions for pos in pattern):\n",
    "                return 'X'\n",
    "            elif all(pos in self.o_positions for pos in pattern):\n",
    "                return 'O'\n",
    "        return 'P'  # No winner yet\n",
    "    \n",
    "    def check_win(self):\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "        \n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        my_positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in my_positions for pos in pattern):\n",
    "                return 10 #you win\n",
    "            elif all(pos in opponent_positions for pos in pattern):\n",
    "                return -10 #opponent wins\n",
    "\n",
    "        return 0 #nobody wins\n",
    "    \n",
    "    \n",
    "    def check_is_winning(self, possible_positions):\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "        \n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in possible_positions for pos in pattern):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "\n",
    "    ### METHODS USED TO CHECK POSITIVE REWARD ###\n",
    "\n",
    "    def check_two_pattern(self, position): # checks wether our move is going to fill a row or a colum with 2 X's or O's\n",
    "        positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "        \n",
    "        for i in range(3):\n",
    "            # Check rows, columns and diags\n",
    "            if (positions.intersection(self.board_rows[i]) == 2 and position in self.board[i]) or (positions.intersection(self.board_cols[i]) == 2 and position in self.board_cols[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return 1\n",
    "            \n",
    "            if i==0 or i==1:\n",
    "               if (positions.intersection(self.board_diag[i]) == 2 and position in self.board_diag[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return 1 \n",
    "        return 0\n",
    "               \n",
    "    def block_opponent(self, position): # tells if we actually are blocking the opponent to win\n",
    "        \n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        #win_patterns = [self.board_rows, self.board_cols, self.board_diag]\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in opponent_positions_tmp for pos in pattern) and position in pattern: # in case the opponent would have already a col/row filled with two values, and we're going to fill the entire rol/col\n",
    "                return 2\n",
    "        return 0\n",
    "    \n",
    "    def create_fork(self): #tells me if my move is going to let me win ! I have two possible way of winning at once, regardless of which action the opponent will take\n",
    "        # I traverse all the blanck spaces (number still not assigned), and i look w.r.t. my positions, if i could win at leat twice\n",
    "        positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "        num_wins = 0\n",
    "\n",
    "        for possible_winning_move in range(1, 10):\n",
    "            if possible_winning_move in self.free_positions:\n",
    "                possible_positions = positions.union({possible_winning_move})\n",
    "\n",
    "                if self.check_is_winning(possible_positions):\n",
    "                    num_wins += 1\n",
    "\n",
    "        if num_wins >= 2:\n",
    "            return 5\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "\n",
    "    ### METHODS USED TO CHECK NEGATIVE REWARD ###\n",
    "\n",
    "    def allow_opponent_two_pattern(self, position):\n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        \n",
    "        for i in range(3):\n",
    "            # Check rows, columns and diags\n",
    "            if (opponent_positions_tmp.intersection(self.board_rows[i]) == 2 and position in self.board[i]) or (opponent_positions_tmp.intersection(self.board_cols[i]) == 2 and position in self.board_cols[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return -1\n",
    "            \n",
    "            if i==0 or i==1:\n",
    "               if (opponent_positions_tmp.intersection(self.board_diag[i]) == 2 and position in self.board_diag[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return -1 \n",
    "        return 0\n",
    "    \n",
    "    def allow_opponnent_fork(self, position):\n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        num_wins = 0\n",
    "\n",
    "        for possible_winning_move in range(1, 10):\n",
    "            if possible_winning_move in self.free_positions:\n",
    "                possible_positions = opponent_positions_tmp.union({possible_winning_move})\n",
    "\n",
    "                if self.check_is_winning(possible_positions):\n",
    "                    num_wins += 1\n",
    "\n",
    "        if num_wins >= 2:\n",
    "            return -5\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of matches we will run\n",
    "n_matches = 100000\n",
    "\n",
    "#initialize the exploration probability\n",
    "exploration_prob = 0.2\n",
    "\n",
    "#discounted factor\n",
    "gamma = 0.95\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = defaultdict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Q_table(Q_table: defaultdict, tic_tac_toe: TicTacToe): #return the state-actions related to a specific state\n",
    "    hashable_state = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "    if hashable_state not in Q_table:\n",
    "        # in case the actual state is not contained inside the Q_table, we insert it and initialize all state-actions to 0.0\n",
    "        Q_table[hashable_state] = list([(action, 0.0) for action in tic_tac_toe.free_positions])\n",
    "        \n",
    "    return Q_table[hashable_state]\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(exploration_prob, state_actions, tic_tac_toe): #based on epsilon, i take a random free position, or the one with the highest value in the Q_table\n",
    "    \n",
    "    if random.random() < exploration_prob:\n",
    "        position = random.choice(list(tic_tac_toe.free_positions)) \n",
    "    else :\n",
    "        position = max(state_actions, key=lambda x: x[1])[0] # take the position (action) with the maximum Q-value\n",
    "\n",
    "    tic_tac_toe.free_positions.remove(position)\n",
    "    tic_tac_toe.make_move(position)\n",
    "    #exploration_prob -= exploration_decreasing_decay\n",
    "\n",
    "    return position\n",
    "\n",
    "def compute_reward(position, tic_tac_toe):\n",
    "    total_reward = 0\n",
    "\n",
    "    total_reward += tic_tac_toe.check_two_pattern(position)\n",
    "    total_reward += tic_tac_toe.block_opponent(position)\n",
    "    total_reward += tic_tac_toe.create_fork()\n",
    "    total_reward += tic_tac_toe.allow_opponent_two_pattern(position)\n",
    "    total_reward += tic_tac_toe.allow_opponnent_fork(position)\n",
    "    total_reward += tic_tac_toe.check_win()\n",
    "\n",
    "    # prova a farlo con una media sul numero di possibili reward\n",
    "    if total_reward == 0: # this means that nothing meaningful happened in the game with our move\n",
    "        return 0.5 # we give a little reward\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def update_Q_table(Q_table, tic_tac_toe, curr_reward, state_actions_curr, position_chosen, hashable_state_curr):\n",
    "    if len(tic_tac_toe.free_positions) != 0:\n",
    "        next_state_actions = check_Q_table(Q_table, tic_tac_toe)  # verifies the next state best action value\n",
    "        #print(f'{next_state_actions}')\n",
    "        expected_return = curr_reward + gamma*(max(next_state_actions, key=lambda x: x[1])[1])\n",
    "        index = next(i for i, v in enumerate(state_actions_curr) if v[0] == position_chosen)\n",
    "\n",
    "        updated_action_val = Q_table[hashable_state_curr][index][1] + lr*(expected_return - Q_table[hashable_state_curr][index][1])\n",
    "        \n",
    "        Q_table[hashable_state_curr][index] = (position_chosen, updated_action_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "def train_model(curr_player):\n",
    "    reward_game_curr = 0\n",
    "    num_iter = 0\n",
    "    tic_tac_toe = TicTacToe(curr_player) # curr player starts first\n",
    "\n",
    "    while len(tic_tac_toe.free_positions) != 0:\n",
    "        num_iter += 1\n",
    "\n",
    "        # 1- CHECK THAT THE EXPECTED ENTRY IS ACTUALLY IN THE Q TABLE, OTHERWISE I CREATE AND INSERT IT\n",
    "        state_actions_curr = check_Q_table(Q_table, tic_tac_toe)\n",
    "        hashable_state_curr = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "        # 2- BASED ON A CERTAIN PROBABILITY THRESHOLD, I'LL CHOOSE A RANDPOM VALUE OR A VALUE FROM THE Q TABLE (EXPLORATION/EXPLOITATION) \n",
    "        position_chosen = epsilon_greedy_policy(exploration_prob, state_actions_curr, tic_tac_toe) #we modify the state with the call to this function\n",
    "\n",
    "        # 3- AT THE END, I COMPUTE THE COST OF MY LAST ACTION\n",
    "        curr_reward = compute_reward(position_chosen, tic_tac_toe)\n",
    "        #print(f'{curr_reward}')\n",
    "        reward_game_curr += curr_reward\n",
    "\n",
    "        # 4- THE Q TABLE IS UPDATED BASED ON THE NEW STATE AND THE OBTAINED REWARD\n",
    "        update_Q_table(Q_table, tic_tac_toe, curr_reward, state_actions_curr, position_chosen, hashable_state_curr)\n",
    "\n",
    "        if tic_tac_toe.check_winner() == 'X' or tic_tac_toe.check_winner() == 'O': # In case someone wins, then stop the game\n",
    "            break\n",
    "    \n",
    "        tic_tac_toe.switch_player()\n",
    "\n",
    "    return reward_game_curr/num_iter\n",
    "\n",
    "\n",
    "def use_model(curr_player):\n",
    "    tic_tac_toe = TicTacToe(curr_player)\n",
    "\n",
    "    while len(tic_tac_toe.free_positions) != 0:\n",
    "        if tic_tac_toe.current_player == 'X':\n",
    "            hashable_state_curr = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "            if hashable_state_curr in Q_table: # NOT ALL THE POSSIBLE STATES COULD BE EMBEDDED INTO THE Q TABLE, SO WE TAKE A RANDOM MOVE ONCE IN A WHILE\n",
    "                position = max(Q_table[hashable_state_curr], key=lambda x: x[1])[0]\n",
    "            else: position = random.choice(list(tic_tac_toe.free_positions)) \n",
    "            \n",
    "            tic_tac_toe.free_positions.remove(position)\n",
    "            tic_tac_toe.make_move(position)\n",
    "        \n",
    "        else:\n",
    "            position = random.choice(list(tic_tac_toe.free_positions))\n",
    "            tic_tac_toe.free_positions.remove(position)\n",
    "            tic_tac_toe.make_move(position)\n",
    "\n",
    "        if tic_tac_toe.check_winner() == 'X' or tic_tac_toe.check_winner() == 'O': # In case someone wins, then stop the game\n",
    "            break\n",
    "    \n",
    "        tic_tac_toe.switch_player()\n",
    "\n",
    "    if tic_tac_toe.check_winner() == 'X':\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "rewards_stats = []\n",
    "\n",
    "for step in range(n_matches):\n",
    "    if step%2 == 0:\n",
    "        rewards_stats.append(train_model('O')) # play starting with O first\n",
    "    else: rewards_stats.append(train_model('X')) # play starting with X first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(frozenset({1, 4}), frozenset({2, 3, 7}))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[247], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_matches_inference):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m----> 6\u001b[0m         wins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43muse_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: wins \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m use_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwins\u001b[38;5;241m/\u001b[39mn_matches_inference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[245], line 42\u001b[0m, in \u001b[0;36muse_model\u001b[1;34m(curr_player)\u001b[0m\n\u001b[0;32m     39\u001b[0m hashable_state_curr \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mfrozenset\u001b[39m(tic_tac_toe\u001b[38;5;241m.\u001b[39mx_positions), \u001b[38;5;28mfrozenset\u001b[39m(tic_tac_toe\u001b[38;5;241m.\u001b[39mo_positions))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#if hashable_state_curr in Q_table:\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[43mQ_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhashable_state_curr\u001b[49m\u001b[43m]\u001b[49m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#else: position = random.choice(list(tic_tac_toe.free_positions)) \u001b[39;00m\n\u001b[0;32m     45\u001b[0m tic_tac_toe\u001b[38;5;241m.\u001b[39mfree_positions\u001b[38;5;241m.\u001b[39mremove(position)\n",
      "\u001b[1;31mKeyError\u001b[0m: (frozenset({1, 4}), frozenset({2, 3, 7}))"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "n_matches_inference = 1000\n",
    "\n",
    "for step in range(n_matches_inference):\n",
    "    if step%2 == 0:\n",
    "        wins += use_model('O')\n",
    "    else: wins += use_model('X')\n",
    "    \n",
    "\n",
    "print(f'Accuracy playing with a random gamer: {wins/n_matches_inference}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(frozenset(), frozenset()) [(1, 2.7018324080179266), (2, 0.11348817147326662), (3, 0.1383442332165407), (4, 0.1291150392799892), (5, 0.10591155070317947), (6, 0.08592876598854662), (7, 0.1125261247882382), (8, 0.08964638332360401), (9, 0.14780829285258756)]\n",
      "(frozenset(), frozenset({1})) [(2, 2.3077655933909216), (3, 0.03217252039645482), (4, 0.06891719894250932), (5, 0.06624803068375555), (6, 0.07192119822840622), (7, 0.0598197518817637), (8, 0.07422138225406047), (9, 0.026986597916574494)]\n",
      "(frozenset({2}), frozenset({1})) [(3, 3.1638384851459977), (4, 0.029755978536717798), (5, 0.04355337251339205), (6, 0.029815759201763818), (7, 0.059807244855265694), (8, 0.029780386343598857), (9, 0.04202126513317955)]\n",
      "(frozenset({2}), frozenset({1, 3})) [(4, 4.847010961743428), (5, -0.5656676021821184), (6, 0.051156214714211404), (7, -0.4302495198154108), (8, 0.03532242151500589), (9, -0.2360139512300903)]\n",
      "(frozenset({2, 4}), frozenset({1, 3})) [(5, 0.04872704809014403), (6, 0.024647525922500003), (7, 0.08701089067712021), (8, 0.044683402044895015), (9, 7.770420500807949)]\n",
      "(frozenset({2, 4}), frozenset({1, 3, 7})) [(5, 0.3861664239503773), (6, 0.0), (8, -0.09836), (9, -0.09855)]\n",
      "(frozenset({9, 2, 4}), frozenset({1, 3, 7})) [(5, 0.199), (6, 0.0), (8, 0.0)]\n",
      "(frozenset({9, 2, 4}), frozenset({1, 3, 5, 7})) [(6, 0.0), (8, 0.0)]\n",
      "(frozenset({1}), frozenset()) [(2, 0.04177437420809878), (3, 0.0363369186386912), (4, 0.055697534164816175), (5, 3.3025175310752997), (6, 0.07099289231793036), (7, 0.04616851644750745), (8, 0.041725132617409115), (9, 0.036333840389743556)]\n",
      "(frozenset({1}), frozenset({5})) [(2, 4.4656041849888615), (3, 0.03878003278603995), (4, 0.03902340281778797), (6, 0.06472784496493625), (7, 0.05827267883738349), (8, 0.024740066990000004), (9, 0.06415901435761678)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, value in itertools.islice(Q_table.items(), 10):\n",
    "    print(key, value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
