{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TicTacToe:\n",
    "    def __init__(self, curr_player):\n",
    "        self.board_rows = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # rows\n",
    "        self.board_cols = [[1, 4, 7], [2, 5, 8], [3, 6, 9]]  # columns\n",
    "        self.board_diag = [[1, 5, 9], [3, 5, 7]]             # diagonals\n",
    "        self.x_positions = set()\n",
    "        self.o_positions = set()\n",
    "        self.free_positions = set(range(1, 10)) # at the beginning they are all free\n",
    "        self.current_player = curr_player\n",
    "\n",
    "    def make_move(self, position):\n",
    "        # Adds a position w.r.t the player taken positions\n",
    "        self.x_positions.add(position) if self.current_player == 'X' else self.o_positions.add(position)\n",
    "\n",
    "\n",
    "\n",
    "    ### ALL THESE FUNCTIONS ARE CALLED ONCE AFTER THE MOVE HAS BEEN DONE ###\n",
    "    def check_winner(self):\n",
    "        # Check rows, columns, and diagonals for a win\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in self.x_positions for pos in pattern):\n",
    "                return 'X'\n",
    "            elif all(pos in self.o_positions for pos in pattern):\n",
    "                return 'O'\n",
    "        return 'P'  # No winner yet\n",
    "    \n",
    "    def check_win(self):\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "        \n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        my_positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in my_positions for pos in pattern):\n",
    "                return 10 #you win\n",
    "            elif all(pos in opponent_positions for pos in pattern):\n",
    "                return -10 #opponent wins\n",
    "\n",
    "        return 0 #nobody wins\n",
    "    \n",
    "    \n",
    "    def check_is_winning(self, possible_positions):\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "        \n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in possible_positions for pos in pattern):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def switch_player(self):\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "\n",
    "\n",
    "    ### METHODS USED TO CHECK POSITIVE REWARD ###\n",
    "\n",
    "    def check_two_pattern(self, position): # checks wether our move is going to fill a row or a colum with 2 X's or O's\n",
    "        positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "        \n",
    "        for i in range(3):\n",
    "            # Check rows, columns and diags\n",
    "            if (positions.intersection(self.board_rows[i]) == 2 and position in self.board[i]) or (positions.intersection(self.board_cols[i]) == 2 and position in self.board_cols[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return 1\n",
    "            \n",
    "            if i==0 or i==1:\n",
    "               if (positions.intersection(self.board_diag[i]) == 2 and position in self.board_diag[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return 1 \n",
    "        return 0\n",
    "               \n",
    "    def block_opponent(self, position): # tells if we actually are blocking the opponent to win\n",
    "        \n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        #win_patterns = [self.board_rows, self.board_cols, self.board_diag]\n",
    "        win_patterns = [tuple(row) for row in self.board_rows] + [tuple(col) for col in self.board_cols] + [tuple(diag) for diag in self.board_diag]\n",
    "\n",
    "        for pattern in win_patterns:\n",
    "            if all(pos in opponent_positions_tmp for pos in pattern) and position in pattern: # in case the opponent would have already a col/row filled with two values, and we're going to fill the entire rol/col\n",
    "                return 2\n",
    "        return 0\n",
    "    \n",
    "    def create_fork(self): #tells me if my move is going to let me win ! I have two possible way of winning at once, regardless of which action the opponent will take\n",
    "        # I traverse all the blanck spaces (number still not assigned), and i look w.r.t. my positions, if i could win at leat twice\n",
    "        positions = self.x_positions if self.current_player == 'X' else self.o_positions\n",
    "        num_wins = 0\n",
    "\n",
    "        for possible_winning_move in range(1, 10):\n",
    "            if possible_winning_move in self.free_positions:\n",
    "                possible_positions = positions.union({possible_winning_move})\n",
    "\n",
    "                if self.check_is_winning(possible_positions):\n",
    "                    num_wins += 1\n",
    "\n",
    "        if num_wins >= 2:\n",
    "            return 5\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "\n",
    "    ### METHODS USED TO CHECK NEGATIVE REWARD ###\n",
    "\n",
    "    def allow_opponent_two_pattern(self, position):\n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        \n",
    "        for i in range(3):\n",
    "            # Check rows, columns and diags\n",
    "            if (opponent_positions_tmp.intersection(self.board_rows[i]) == 2 and position in self.board[i]) or (opponent_positions_tmp.intersection(self.board_cols[i]) == 2 and position in self.board_cols[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return -1\n",
    "            \n",
    "            if i==0 or i==1:\n",
    "               if (opponent_positions_tmp.intersection(self.board_diag[i]) == 2 and position in self.board_diag[i]): # if the position I choose belong to a 2-value column/row or diagonal, then retrieve the reward\n",
    "                return -1 \n",
    "        return 0\n",
    "    \n",
    "    def allow_opponnent_fork(self, position):\n",
    "        opponent_positions = self.o_positions if self.current_player == 'X' else self.x_positions\n",
    "        opponent_positions_tmp = opponent_positions.union({position})\n",
    "        num_wins = 0\n",
    "\n",
    "        for possible_winning_move in range(1, 10):\n",
    "            if possible_winning_move in self.free_positions:\n",
    "                possible_positions = opponent_positions_tmp.union({possible_winning_move})\n",
    "\n",
    "                if self.check_is_winning(possible_positions):\n",
    "                    num_wins += 1\n",
    "\n",
    "        if num_wins >= 2:\n",
    "            return -5\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEMENT LEARNING ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of matches we will run\n",
    "n_matches = 100000\n",
    "\n",
    "#initialize the exploration probability\n",
    "exploration_prob = 0.2\n",
    "\n",
    "#discounted factor\n",
    "gamma = 0.95\n",
    "\n",
    "#learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Initialize Q-table\n",
    "Q_table = defaultdict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Q_table(Q_table: defaultdict, tic_tac_toe: TicTacToe): #return the state-actions related to a specific state\n",
    "    hashable_state = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "    if hashable_state not in Q_table:\n",
    "        # in case the actual state is not contained inside the Q_table, we insert it and initialize all state-actions to 0.0\n",
    "        Q_table[hashable_state] = list([(action, 0.0) for action in tic_tac_toe.free_positions])\n",
    "        \n",
    "    return Q_table[hashable_state]\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(exploration_prob, state_actions, tic_tac_toe): #based on epsilon, i take a random free position, or the one with the highest value in the Q_table\n",
    "    \n",
    "    if random.random() < exploration_prob:\n",
    "        position = random.choice(list(tic_tac_toe.free_positions)) \n",
    "    else :\n",
    "        position = max(state_actions, key=lambda x: x[1])[0] # take the position (action) with the maximum Q-value\n",
    "\n",
    "    tic_tac_toe.free_positions.remove(position)\n",
    "    tic_tac_toe.make_move(position)\n",
    "    #exploration_prob -= exploration_decreasing_decay\n",
    "\n",
    "    return position\n",
    "\n",
    "def compute_reward(position, tic_tac_toe):\n",
    "    total_reward = 0\n",
    "\n",
    "    total_reward += tic_tac_toe.check_two_pattern(position)\n",
    "    total_reward += tic_tac_toe.block_opponent(position)\n",
    "    total_reward += tic_tac_toe.create_fork()\n",
    "    total_reward += tic_tac_toe.allow_opponent_two_pattern(position)\n",
    "    total_reward += tic_tac_toe.allow_opponnent_fork(position)\n",
    "    total_reward += tic_tac_toe.check_win()\n",
    "\n",
    "    # prova a farlo con una media sul numero di possibili reward\n",
    "    if total_reward == 0: # this means that nothing meaningful happened in the game with our move\n",
    "        return 0.5 # we give a little reward\n",
    "    \n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def update_Q_table(Q_table, tic_tac_toe, curr_reward, state_actions_curr, position_chosen, hashable_state_curr):\n",
    "    if len(tic_tac_toe.free_positions) != 0:\n",
    "        next_state_actions = check_Q_table(Q_table, tic_tac_toe)  # verifies the next state best action value\n",
    "        #print(f'{next_state_actions}')\n",
    "        expected_return = curr_reward + gamma*(max(next_state_actions, key=lambda x: x[1])[1])\n",
    "        index = next(i for i, v in enumerate(state_actions_curr) if v[0] == position_chosen)\n",
    "\n",
    "        updated_action_val = Q_table[hashable_state_curr][index][1] + lr*(expected_return - Q_table[hashable_state_curr][index][1])\n",
    "        \n",
    "        Q_table[hashable_state_curr][index] = (position_chosen, updated_action_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "def train_model(curr_player):\n",
    "    reward_game_curr = 0\n",
    "    num_iter = 0\n",
    "    tic_tac_toe = TicTacToe(curr_player) # curr player starts first\n",
    "\n",
    "    while len(tic_tac_toe.free_positions) != 0:\n",
    "        num_iter += 1\n",
    "\n",
    "        # 1- CHECK THAT THE EXPECTED ENTRY IS ACTUALLY IN THE Q TABLE, OTHERWISE I CREATE AND INSERT IT\n",
    "        state_actions_curr = check_Q_table(Q_table, tic_tac_toe)\n",
    "        hashable_state_curr = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "        # 2- BASED ON A CERTAIN PROBABILITY THRESHOLD, I'LL CHOOSE A RANDPOM VALUE OR A VALUE FROM THE Q TABLE (EXPLORATION/EXPLOITATION) \n",
    "        position_chosen = epsilon_greedy_policy(exploration_prob, state_actions_curr, tic_tac_toe) #we modify the state with the call to this function\n",
    "\n",
    "        # 3- AT THE END, I COMPUTE THE COST OF MY LAST ACTION\n",
    "        curr_reward = compute_reward(position_chosen, tic_tac_toe)\n",
    "        #print(f'{curr_reward}')\n",
    "        reward_game_curr += curr_reward\n",
    "\n",
    "        # 4- THE Q TABLE IS UPDATED BASED ON THE NEW STATE AND THE OBTAINED REWARD\n",
    "        update_Q_table(Q_table, tic_tac_toe, curr_reward, state_actions_curr, position_chosen, hashable_state_curr)\n",
    "\n",
    "        if tic_tac_toe.check_winner() == 'X' or tic_tac_toe.check_winner() == 'O': # In case someone wins, then stop the game\n",
    "            break\n",
    "    \n",
    "        tic_tac_toe.switch_player()\n",
    "\n",
    "    return reward_game_curr/num_iter\n",
    "\n",
    "\n",
    "def use_model(curr_player):\n",
    "    tic_tac_toe = TicTacToe(curr_player)\n",
    "\n",
    "    while len(tic_tac_toe.free_positions) != 0:\n",
    "        if tic_tac_toe.current_player == 'X':\n",
    "            hashable_state_curr = (frozenset(tic_tac_toe.x_positions), frozenset(tic_tac_toe.o_positions))\n",
    "\n",
    "            if hashable_state_curr in Q_table: # NOT ALL THE POSSIBLE STATES COULD BE EMBEDDED INTO THE Q TABLE, SO WE TAKE A RANDOM MOVE ONCE IN A WHILE\n",
    "                position = max(Q_table[hashable_state_curr], key=lambda x: x[1])[0]\n",
    "            else: position = random.choice(list(tic_tac_toe.free_positions)) \n",
    "            \n",
    "            tic_tac_toe.free_positions.remove(position)\n",
    "            tic_tac_toe.make_move(position)\n",
    "        \n",
    "        else:\n",
    "            position = random.choice(list(tic_tac_toe.free_positions))\n",
    "            tic_tac_toe.free_positions.remove(position)\n",
    "            tic_tac_toe.make_move(position)\n",
    "\n",
    "        if tic_tac_toe.check_winner() == 'X' or tic_tac_toe.check_winner() == 'O': # In case someone wins, then stop the game\n",
    "            break\n",
    "    \n",
    "        tic_tac_toe.switch_player()\n",
    "\n",
    "    if tic_tac_toe.check_winner() == 'X':\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "rewards_stats = []\n",
    "\n",
    "for step in range(n_matches):\n",
    "    if step%2 == 0:\n",
    "        rewards_stats.append(train_model('O')) # play starting with O first\n",
    "    else: rewards_stats.append(train_model('X')) # play starting with X first\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy playing with a random gamer: 0.718\n"
     ]
    }
   ],
   "source": [
    "wins = 0 \n",
    "n_matches_inference = 1000\n",
    "\n",
    "for step in range(n_matches_inference):\n",
    "    if step%2 == 0:\n",
    "        wins += use_model('O')          \n",
    "    else: wins += use_model('X')\n",
    "    \n",
    "\n",
    "print(f'Accuracy playing with a random gamer: {wins/n_matches_inference}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(frozenset(), frozenset()) [(1, 16.511233098379996), (2, 13.264024049876095), (3, 11.806731712284375), (4, 9.894354189959099), (5, 9.38981471745391), (6, 10.387484106089376), (7, 10.658189390722166), (8, 11.119415794956796), (9, 10.936421767621058)]\n",
      "(frozenset(), frozenset({1})) [(2, 9.621842116450242), (3, 16.216555037535446), (4, 8.467994690983092), (5, 9.130541669746316), (6, 6.925915894225976), (7, 9.993067625250026), (8, 11.43101964734717), (9, 10.780756780388629)]\n",
      "(frozenset({3}), frozenset({1})) [(2, 16.508421969562338), (4, 12.236252881742441), (5, 6.259631519253611), (6, 15.325586923556317), (7, 12.995621508709618), (8, 11.813560275490834), (9, 14.697150332415449)]\n",
      "(frozenset({3}), frozenset({1, 2})) [(4, 16.976324090439675), (5, 4.580828825842993), (6, 11.988136822898475), (7, 15.250304794570345), (8, 12.202154789041087), (9, 9.491113829937804)]\n",
      "(frozenset({3, 5}), frozenset({1, 2})) [(4, -0.9044137581866071), (6, -0.733009841046057), (7, -0.2115267118609852), (8, 10.228667207551789), (9, -1.1868913808082524)]\n",
      "(frozenset({3, 5}), frozenset({1, 2, 4})) [(6, 0.05449205486000001), (7, 4.132089353110274), (8, 0.09414587544793762), (9, 0.15763164984822683)]\n",
      "(frozenset({3, 5, 7}), frozenset({1, 2, 4})) [(6, 0.0), (8, 0.0), (9, 0.0)]\n",
      "(frozenset({1}), frozenset()) [(2, 17.51954703598889), (3, 7.111434828177667), (4, 9.383698202255356), (5, 10.31915042340443), (6, 8.389709050080443), (7, 12.462603965965087), (8, 11.387151214980134), (9, 12.002607580805394)]\n",
      "(frozenset({1}), frozenset({2})) [(3, 17.901790206451526), (4, 10.061702384538513), (5, 12.119342085594623), (6, 14.363575035209283), (7, 16.254543666888477), (8, 11.377612967060594), (9, 12.36561147416315)]\n",
      "(frozenset({1, 3}), frozenset({2})) [(4, 18.39281767468603), (5, 6.747094665783999), (6, 16.153436073953284), (7, 8.64411352738986), (8, 9.739069798916715), (9, 7.509837209570108)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for key, value in itertools.islice(Q_table.items(), 10):\n",
    "    print(key, value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
