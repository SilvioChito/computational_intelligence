{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random, numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer,translate_number_to_position_direction,translate_number_to_position,TrainedPlayer\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genome:\n",
    "    def __init__(self, ):\n",
    "        self.genotype = create_rand_gen()\n",
    "        self.fitness = float(\"-inf\")\n",
    "\n",
    "cell_max_values = [\n",
    "    (1, 8),    # num_matches\n",
    "    (100, 1_000),  # max_dim_replay_buff\n",
    "    (2, 80),   # time_to_update\n",
    "    (0.2, 1.0),   # gamma\n",
    "    (50, 500)    # batch_size\n",
    "]\n",
    "\n",
    "def create_rand_gen(): # creates the genome, referred to the scale of our values we'd like to estimate    \n",
    "    # Define the maximum values for each cell\n",
    "    # Generate a random vector\n",
    "    genome = [random.uniform(cell[0], cell[1]) if isinstance(cell[0], float) else random.randint(cell[0], cell[1]) for cell in cell_max_values]\n",
    "    genome[4] = min(genome[1], genome[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "    return genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POTREMMO ANCHE PRENDERLA DAL FILE training_phase.ipynb\n",
    "class ReplayBuffer:     \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(individual: 'Genome', starts_first: bool):\n",
    "    ## Constants definition\n",
    "    num_iterations = 100\n",
    "    num_matches = individual.genotype[0]\n",
    "    max_dim_replay_buff = individual.genotype[1]\n",
    "    time_to_update = individual.genotype[2]\n",
    "    gamma = individual.genotype[3]\n",
    "    batch_size = individual.genotype[4]\n",
    "    # potentially also the learning rate\n",
    "    agent_to_train = MyPlayer()\n",
    "    opponent = RandomPlayer()\n",
    "\n",
    "    loss_tot = []\n",
    "    \n",
    "    taboo_set = set()\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    # inizializza i modelli -> passali per parametro\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "\n",
    "    \n",
    "    for step in range(num_iterations):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        #player1.step=step\n",
    "        for match in range(num_matches):\n",
    "            # lo uso solo in inferenza\n",
    "            # gioca una intera partita qui dentro e per ogni mossa, metti l'experience dentro il ReplayBuffer\n",
    "            # ...\n",
    "            # ... qui descrivere come viene effettivamente usata la rete neurale (magari impacchettarla in qualche funzione)\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            #g.current_player_idx = match%2\n",
    "            g.current_player_idx = int(starts_first)  #parto per secondo\n",
    "\n",
    "            #player1 = MyPlayer()\n",
    "            #player2 = RandomPlayer()\n",
    "            players = [agent_to_train, opponent]\n",
    "\n",
    "            go = False # serve a calcolare lo stato successivo per il calcolo del target\n",
    "\n",
    "            while winner < 0:\n",
    "                    g.current_player_idx += 1\n",
    "                    g.current_player_idx %= len(players)\n",
    "                    prev_state=deepcopy(g)\n",
    "                    from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                    g._Game__move(from_pos, slide, g.current_player_idx)\n",
    "\n",
    "                    \n",
    "                    if g.current_player_idx==0:\n",
    "                        ##l'azione Ã¨ il valore del q-value quindi un numero? o la posizione in questo caso\n",
    "                        ##in quel caso replay_row=(prev_state.get_board,GeneratorNet((prev_state.get_board),g.get_board,reward)\n",
    "                        reward=g.compute_reward(from_pos, slide)\n",
    "                        go = True\n",
    "                        #io qua dentro x ogni row voglio i 44 output della mia rete\n",
    "                        \n",
    "                        #replay_row=(prev_state.get_flat_board(),player1.last_action_number, deepcopy(g), reward)\n",
    "\n",
    "                        # faccio fare una mossa all'avversario e poi faccio il max action sul nuovo stato che mi ritorna\n",
    "                        \n",
    "                    elif go and g.current_player_idx==1:\n",
    "\n",
    "                        #io qua dentro x ogni row voglio i 44 output della mia rete\n",
    "                        if (tuple(prev_state.get_flat_board()), agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())) not in taboo_set :\n",
    "                            replay_row=(prev_state.get_flat_board(), agent_to_train.last_action_number, deepcopy(g), reward, reward==1)\n",
    "                            taboo_set.add((tuple(prev_state.get_flat_board()),agent_to_train.last_action_number, tuple(deepcopy(g).get_flat_board())))\n",
    "                            #eliminato last_action da replay row?? Non dovrebbe servire\n",
    "                            replay_buff.add_experience(replay_row)\n",
    "\n",
    "                        #replay_buff.add_experience(replay_row)                        \n",
    "\n",
    "                    if g.check_winner() != -1:\n",
    "                        break\n",
    "            \n",
    "            # replay_buffer.push(...) per ogni transizione di stato osservata ()\n",
    "            # restituisci il risultato sotto forma di tupla (st, a, st+1, r) - (State, action, next_state, reward)\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(batch_size)\n",
    "      \n",
    "        #divide the batch\n",
    "        state_batch, action_num, next_state_batch, reward, done = zip(*batch_to_train) \n",
    "\n",
    "        #forward the Generator\n",
    "        q_values = agent_to_train.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(agent_to_train.device)\n",
    "        q_values_target = torch.zeros(batch_size, 44).to(agent_to_train.device)\n",
    "\n",
    "\n",
    "        #update q_values target by using Bellman Equation  \n",
    "        for i in range(batch_size): \n",
    "            if done[i] == False:\n",
    "                q_values_target[i, action_num[i]] = reward[i] + gamma * torch.tensor(agent_to_train.compute_target(next_state_batch[i])).to(agent_to_train.device).item()\n",
    "            else:\n",
    "               q_values_target[i, action_num[i]] = reward[i] \n",
    "          \n",
    "        agent_to_train.optimizer.zero_grad()\n",
    "        loss_curr=agent_to_train.criterion(q_values,q_values_target).to(agent_to_train.device)\n",
    "        loss_curr.backward()\n",
    "    \n",
    "        agent_to_train.optimizer.step()\n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "      \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            agent_to_train.copy_params_TargetNet()\n",
    "\n",
    "        # si dovrebbe aggiungere alla struttura del Genoma anche il dizionario ed la loss_tot\n",
    "            \n",
    "    return  agent_to_train.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 10\n",
    "OFFSPRING_SIZE = 4\n",
    "LOCI = 5\n",
    "MUTATION_PROBABILITY = 0.5\n",
    "BIT_MUTATION_PROBABILITY = 0.5\n",
    "NUM_GENERATION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def new_offspring(parent1: 'Genome', parent2: 'Genome') -> 'Genome':\n",
    "    if random.random() < MUTATION_PROBABILITY:\n",
    "        return mutate(parent1)\n",
    "    return three_cut_xover(parent1, parent2)\n",
    "    \n",
    "def mutate(parent: 'Genome') -> 'Genome':\n",
    "    new_offspring = deepcopy(parent)\n",
    "\n",
    "    # Apply bit flip to the shuffled elements\n",
    "    for _ in range(LOCI):\n",
    "        if random.random() < BIT_MUTATION_PROBABILITY:\n",
    "            index_to_mutate = randint(0, LOCI-1)\n",
    "            new_offspring.genotype[index_to_mutate] = random.uniform(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1]) if isinstance(cell_max_values[index_to_mutate][0], float) else random.randint(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1])\n",
    "            new_offspring.genotype[4] = min(new_offspring.genotype[1], new_offspring.genotype[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "    \n",
    "    return new_offspring\n",
    "\n",
    "def three_cut_xover(ind1: 'Genome', ind2: 'Genome') -> 'Genome':\n",
    "    one_cut_point = randint(0, int((LOCI)*0.3))\n",
    "    two_cut_point = randint(int((LOCI)*0.3), int((LOCI)*0.6))\n",
    "    three_cut_point = randint(int((LOCI)*0.6), LOCI - 1)\n",
    "  \n",
    "    # Order the cut points\n",
    "    cut_points = sorted([one_cut_point, two_cut_point, three_cut_point])\n",
    "    \n",
    "    new_ind = Genome()\n",
    "    new_ind.genotype = (ind1.genotype[:cut_points[0]] +\n",
    "                        ind2.genotype[cut_points[0]:cut_points[1]] +\n",
    "                        ind1.genotype[cut_points[1]:cut_points[2]] +\n",
    "                        ind2.genotype[cut_points[2]:])\n",
    "    \n",
    "    new_ind.genotype[4] = min(new_ind.genotype[1], new_ind.genotype[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "    \n",
    "    assert len(new_ind.genotype) == LOCI\n",
    "    return new_ind\n",
    "\n",
    "def compute_fitness(individual: 'Genome', starts_first: bool):\n",
    "    TrainedGeneratorNet = TrainedPlayer()\n",
    "    trained_weights = training(individual, starts_first)\n",
    "    TrainedGeneratorNet.GeneratorNet.load_state_dict(trained_weights) # TRAINED NETWORK to use in INFERENCE PHASE\n",
    "\n",
    "    wins = 0\n",
    "    num_match_test = 100\n",
    "\n",
    "    for step in range(num_match_test):\n",
    "        player=RandomPlayer()\n",
    "        g=Game()\n",
    "        #g.current_player_idx = step%2\n",
    "        g.current_player_idx = int(starts_first) # in base al valore che \n",
    "        winner=g.play(TrainedGeneratorNet, player)\n",
    "        \n",
    "        if winner==0:\n",
    "            wins+=1\n",
    "\n",
    "    individual.fitness = (wins/num_match_test)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genetic_algorithm(starts_first: bool):\n",
    "        population = [Genome() for _ in range(POPULATION_SIZE)] # starting pouplation of POPULATION_SIZE individuals\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "                compute_fitness(population[i], starts_first)\n",
    "\n",
    "        population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "        best_fitness = population[0].fitness\n",
    "        best_individual = population[0]\n",
    "        gen = 0\n",
    "\n",
    "        while  gen < NUM_GENERATION:                \n",
    "                for _ in range(OFFSPRING_SIZE):\n",
    "                        offspring = new_offspring(population[0], population[1])\n",
    "                        compute_fitness(offspring, starts_first)\n",
    "\n",
    "                population.extend([offspring])\n",
    "                population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "                population = population[:POPULATION_SIZE]\n",
    "                best_individual = population[0]\n",
    "                best_fitness = population[0].fitness\n",
    "                gen += 1\n",
    "                print(f\"Best individual (until now): {best_individual.genotype}\")\n",
    "                print(f\"Best fitness: {best_fitness}\")\n",
    "\n",
    "\n",
    "        print(f\"Best individual with fitness: {best_fitness}\")\n",
    "        \n",
    "        return best_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "training() missing 1 required positional argument: 'starts_first'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BEST_INDIVIDUAL_FIRST \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m BEST_INDIVIDUAL_SECOND \u001b[38;5;241m=\u001b[39m genetic_algorithm(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[1;34m(starts_first)\u001b[0m\n\u001b[0;32m      2\u001b[0m population \u001b[38;5;241m=\u001b[39m [Genome() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(POPULATION_SIZE)] \u001b[38;5;66;03m# starting pouplation of POPULATION_SIZE individuals\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(OFFSPRING_SIZE):\n\u001b[1;32m----> 4\u001b[0m         \u001b[43mcompute_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m population\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m i: i\u001b[38;5;241m.\u001b[39mfitness, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m best_fitness \u001b[38;5;241m=\u001b[39m population[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfitness\n",
      "Cell \u001b[1;32mIn[6], line 45\u001b[0m, in \u001b[0;36mcompute_fitness\u001b[1;34m(individual, starts_first)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_fitness\u001b[39m(individual: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenome\u001b[39m\u001b[38;5;124m'\u001b[39m, starts_first: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m     44\u001b[0m     TrainedGeneratorNet \u001b[38;5;241m=\u001b[39m TrainedPlayer()\n\u001b[1;32m---> 45\u001b[0m     trained_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     TrainedGeneratorNet\u001b[38;5;241m.\u001b[39mGeneratorNet\u001b[38;5;241m.\u001b[39mload_state_dict(trained_weights) \u001b[38;5;66;03m# TRAINED NETWORK to use in INFERENCE PHASE\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: training() missing 1 required positional argument: 'starts_first'"
     ]
    }
   ],
   "source": [
    "BEST_INDIVIDUAL_FIRST = genetic_algorithm(True)\n",
    "BEST_INDIVIDUAL_SECOND = genetic_algorithm(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best individual (starts first turn of the game): {BEST_INDIVIDUAL_FIRST.genotype}\")\n",
    "print(f\"Best individual (starts second turn of the game): {BEST_INDIVIDUAL_SECOND.genotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
