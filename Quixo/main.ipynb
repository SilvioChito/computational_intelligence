{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random,numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer, TrainedPlayer, translate_number_to_position_direction, translate_number_to_position\n",
    "import torch.nn.init as init\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuixoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5*5, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 44),\n",
    "        )\n",
    "        for layer in self.linear_relu_stack:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    init.xavier_uniform_(layer.weight)\n",
    "                    layer.bias.data.fill_(0.01) # bias is initialized to 0.01 \n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silvio/anaconda3/envs/myenv/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392035891/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Constants definition\n",
    "num_iterations = 500\n",
    "num_matches = 10\n",
    "max_dim_replay_buff = 10_000\n",
    "time_to_update = 100\n",
    "gamma = 0.99\n",
    "batch_size=50\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(player1: 'Player', player2: 'Player'):\n",
    "    loss_tot = []\n",
    "   \n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    # inizializza i modelli -> passali per parametro\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    \n",
    "    for step in tqdm(range(num_iterations), desc=\"Training Iterations\"):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "    \n",
    "        for match in range(num_matches):\n",
    "            # lo uso solo in inferenza\n",
    "            # gioca una intera partita qui dentro e per ogni mossa, metti l'experience dentro il ReplayBuffer\n",
    "            # ...\n",
    "            # ... qui descrivere come viene effettivamente usata la rete neurale (magari impacchettarla in qualche funzione)\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            #player1 = MyPlayer()\n",
    "            #player2 = RandomPlayer()\n",
    "            players = [player1, player2]\n",
    "            \n",
    "            while winner < 0:\n",
    "              \n",
    "                    g.current_player_idx += 1\n",
    "                    g.current_player_idx %= len(players)\n",
    "                    #print(g.current_player_idx)\n",
    "                    prev_state=deepcopy(g)\n",
    "                    from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                    \n",
    "                    if g.current_player_idx==0:\n",
    "                        ##l'azione è il valore del q-value quindi un numero? o la posizione in questo caso  \n",
    "                        ##in quel caso replay_row=(prev_state.get_board,GeneratorNet((prev_state.get_board),g.get_board,reward)\n",
    "                        reward=g.compute_reward(from_pos, slide)\n",
    "                        #io qua dentro x ogni row voglio i 44 output della mia rete\n",
    "                        \n",
    "                        replay_row=(prev_state.get_flat_board(),player1.last_action_number, deepcopy(g), reward)\n",
    "                        #eliminato last_action da replay row?? Non dovrebbe servire\n",
    "                        replay_buff.add_experience(replay_row)\n",
    "                        \n",
    "                    \n",
    "                    if g.check_winner() != -1:\n",
    "                        break\n",
    "            \n",
    "            # replay_buffer.push(...) per ogni transizione di stato osservata ()\n",
    "            # restituisci il risultato sotto forma di tupla (st, a, st+1, r) - (State, action, next_state, reward)\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(batch_size)\n",
    "      \n",
    "        #divide the batch\n",
    "        state_batch,action_num, next_state_batch, reward = zip(*batch_to_train) \n",
    "\n",
    "        #forward the Generator\n",
    "        q_values = player1.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32).to(player1.device))\n",
    "        q_values_target = torch.zeros(batch_size, 44).to(player1.device)\n",
    "\n",
    "\n",
    "        #update q_values target by using Bellman Equation  \n",
    "        for i in range(batch_size): \n",
    "            if reward[i]!=1:\n",
    "                q_values_target[i, action_num[i]] = reward[i] + gamma * torch.tensor(player1.compute_target(next_state_batch[i])).to(player1.device).item()\n",
    "            else:\n",
    "               q_values_target[i, action_num[i]] =reward[i]\n",
    "          \n",
    "        player1.optimizer.zero_grad()\n",
    "        loss_curr=player1.criterion(q_values,q_values_target)\n",
    "        loss_curr.backward()\n",
    "    \n",
    "        player1.optimizer.step()\n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "      \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            player1.copy_params_TargetNet()\n",
    "            \n",
    "       \n",
    "        #printa come varia \n",
    "   \n",
    "    return player1.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Iterations:   0%|          | 0/500 [00:00<?, ?it/s]/tmp/ipykernel_38282/1612721276.py:59: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1695392035891/work/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  q_values = player1.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32).to(player1.device))\n",
      "Training Iterations: 100%|██████████| 500/500 [00:32<00:00, 15.45it/s]\n"
     ]
    }
   ],
   "source": [
    "player1 = MyPlayer()\n",
    "player2 = RandomPlayer()\n",
    "#TrainedGeneratorNet=MyPlayer()\n",
    "\n",
    "\n",
    "trained_model_params = training(player1, player2)\n",
    "trained_player = TrainedPlayer(trained_model_params)\n",
    "#TrainedGeneratorNet.GeneratorNet.load_state_dict(trained_model_params) # TRAINED NETWORK to use in INFERENCE PHASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 355.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sii\n",
      "noo\n",
      "noo\n",
      "noo\n",
      "sii\n",
      "sii\n",
      "sii\n",
      "sii\n",
      "sii\n",
      "sii\n",
      "Accuracy: 70.0\n",
      "Wins: 7 - Losts: 3 - Draws 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wins = 0\n",
    "losts = 0\n",
    "draws = 0\n",
    "num_match_test = 10\n",
    "\n",
    "for _ in tqdm(range(num_match_test)):\n",
    "    player=RandomPlayer()\n",
    "    g=Game()\n",
    "    #winner=g.play(TrainedGeneratorNet,player)\n",
    "    winner=g.play(trained_player,player)\n",
    "\n",
    "    if winner==0:\n",
    "        wins+=1\n",
    "        print(\"sii\")\n",
    "       \n",
    "    if winner==1:\n",
    "        print(\"noo\")\n",
    "        losts+=1\n",
    "        \n",
    "    if winner==-2:\n",
    "        print(\"bho\")\n",
    "        draws+=1\n",
    "\n",
    "print(f\"Accuracy: {(wins/num_match_test)*100}\")\n",
    "print(f\"Wins: {wins} - Losts: {losts} - Draws {draws}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
