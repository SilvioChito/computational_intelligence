{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random, numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer,translate_number_to_position_direction,translate_number_to_position,TrainedPlayer\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Genome:\n",
    "    def __init__(self, ):\n",
    "        self.genotype = create_rand_gen()\n",
    "        self.fitness = float(\"-inf\")\n",
    "\n",
    "cell_max_values = [\n",
    "    (1, 8),    # num_matches\n",
    "    (100, 1_000),  # max_dim_replay_buff\n",
    "    (2, 80),   # time_to_update\n",
    "    (0.2, 1.0),   # gamma\n",
    "    (50, 500)    # batch_size\n",
    "]\n",
    "\n",
    "def create_rand_gen(): # i create the genome referred to the scale of our values we'd like to estimate    \n",
    "    # Define the maximum values for each cell\n",
    "    # Generate a random vector\n",
    "    genome = [random.uniform(cell[0], cell[1]) if isinstance(cell[0], float) else random.randint(cell[0], cell[1]) for cell in cell_max_values]\n",
    "\n",
    "    genome[4] = min(genome[1], genome[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "\n",
    "    return genome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        batch_indices = np.random.choice(len(self.buffer), batch_size, replace=True)\n",
    "        batch = [self.buffer[i] for i in batch_indices]\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def training(individual: 'Genome'):\n",
    "    ## Constants definition\n",
    "    num_iterations = 200\n",
    "    num_matches = individual.genotype[0]\n",
    "    max_dim_replay_buff = individual.genotype[1]\n",
    "    time_to_update = individual.genotype[2]\n",
    "    gamma = individual.genotype[3]\n",
    "    batch_size = individual.genotype[4]\n",
    "    # potentially also the learning rate\n",
    "    player1 = MyPlayer()\n",
    "    player2 = RandomPlayer()\n",
    "\n",
    "    loss_tot = []\n",
    "    \n",
    "    taboo_set = set()\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    # inizializza i modelli -> passali per parametro\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "\n",
    "    \n",
    "    for step in range(num_iterations):  \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        #player1.step=step\n",
    "        for match in range(num_matches):\n",
    "            # lo uso solo in inferenza\n",
    "            # gioca una intera partita qui dentro e per ogni mossa, metti l'experience dentro il ReplayBuffer\n",
    "            # ...\n",
    "            # ... qui descrivere come viene effettivamente usata la rete neurale (magari impacchettarla in qualche funzione)\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            #g.current_player_idx = match%2\n",
    "            g.current_player_idx = 1\n",
    "\n",
    "            #player1 = MyPlayer()\n",
    "            #player2 = RandomPlayer()\n",
    "            players = [player1, player2]\n",
    "\n",
    "            go = False # serve a calcolare lo stato successivo per il calcolo del target\n",
    "\n",
    "            while winner < 0:\n",
    "                    g.current_player_idx += 1\n",
    "                    g.current_player_idx %= len(players)\n",
    "                    prev_state=deepcopy(g)\n",
    "                    from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                    \n",
    "                    if g.current_player_idx==0:\n",
    "                        ##l'azione Ã¨ il valore del q-value quindi un numero? o la posizione in questo caso\n",
    "                        ##in quel caso replay_row=(prev_state.get_board,GeneratorNet((prev_state.get_board),g.get_board,reward)\n",
    "                        reward=g.compute_reward(from_pos, slide)\n",
    "                        go = True\n",
    "                        #io qua dentro x ogni row voglio i 44 output della mia rete\n",
    "                        \n",
    "                        #replay_row=(prev_state.get_flat_board(),player1.last_action_number, deepcopy(g), reward)\n",
    "\n",
    "                        # faccio fare una mossa all'avversario e poi faccio il max action sul nuovo stato che mi ritorna\n",
    "                        \n",
    "                    elif go and g.current_player_idx==1:\n",
    "\n",
    "                        #io qua dentro x ogni row voglio i 44 output della mia rete\n",
    "                        if (tuple(prev_state.get_flat_board()), player1.last_action_number, tuple(deepcopy(g).get_flat_board())) not in taboo_set :\n",
    "                            replay_row=(prev_state.get_flat_board(), player1.last_action_number, deepcopy(g), reward, reward==1)\n",
    "                            taboo_set.add((tuple(prev_state.get_flat_board()),player1.last_action_number, tuple(deepcopy(g).get_flat_board())))\n",
    "                            #eliminato last_action da replay row?? Non dovrebbe servire\n",
    "                            replay_buff.add_experience(replay_row)\n",
    "\n",
    "                        #replay_buff.add_experience(replay_row)                        \n",
    "\n",
    "                    if g.check_winner() != -1:\n",
    "                        break\n",
    "            \n",
    "            # replay_buffer.push(...) per ogni transizione di stato osservata ()\n",
    "            # restituisci il risultato sotto forma di tupla (st, a, st+1, r) - (State, action, next_state, reward)\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(batch_size)\n",
    "      \n",
    "        #divide the batch\n",
    "        state_batch, action_num, next_state_batch, reward, done = zip(*batch_to_train) \n",
    "\n",
    "        #forward the Generator\n",
    "        q_values = player1.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(player1.device)\n",
    "        q_values_target = torch.zeros(batch_size, 44).to(player1.device)\n",
    "\n",
    "\n",
    "        #update q_values target by using Bellman Equation  \n",
    "        for i in range(batch_size): \n",
    "            if done[i] == False:\n",
    "                q_values_target[i, action_num[i]] = reward[i] + gamma * torch.tensor(player1.compute_target(next_state_batch[i])).to(player1.device).item()\n",
    "            else:\n",
    "               q_values_target[i, action_num[i]] = reward[i] \n",
    "          \n",
    "        player1.optimizer.zero_grad()\n",
    "        loss_curr=player1.criterion(q_values,q_values_target).to(player1.device)\n",
    "        loss_curr.backward()\n",
    "    \n",
    "        player1.optimizer.step()\n",
    "\n",
    "        loss_tot.append(loss_curr)\n",
    "      \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            player1.copy_params_TargetNet()\n",
    "\n",
    "        # si dovrebbe aggiungere alla struttura del Genoma anche il dizionario ed la loss_tot\n",
    "            \n",
    "    return  player1.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 10\n",
    "OFFSPRING_SIZE = 4\n",
    "LOCI = 5\n",
    "MUTATION_PROBABILITY = 0.5\n",
    "BIT_MUTATION_PROBABILITY = 0.5\n",
    "NUM_GENERATION = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "def new_offspring(parent1: 'Genome', parent2: 'Genome') -> 'Genome':\n",
    "    if random.random() < MUTATION_PROBABILITY:\n",
    "        return mutate(parent1)\n",
    "    return three_cut_xover(parent1, parent2)\n",
    "    \n",
    "def mutate(parent: 'Genome') -> 'Genome':\n",
    "    new_offspring = deepcopy(parent)\n",
    "\n",
    "    # Apply bit flip to the shuffled elements\n",
    "    for _ in range(LOCI):\n",
    "        if random.random() < BIT_MUTATION_PROBABILITY:\n",
    "            index_to_mutate = randint(0, LOCI-1)\n",
    "            new_offspring.genotype[index_to_mutate] = random.uniform(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1]) if isinstance(cell_max_values[index_to_mutate][0], float) else random.randint(cell_max_values[index_to_mutate][0], cell_max_values[index_to_mutate][1])\n",
    "            new_offspring.genotype[4] = min(new_offspring.genotype[1], new_offspring.genotype[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "    \n",
    "    return new_offspring\n",
    "\n",
    "def three_cut_xover(ind1: 'Genome', ind2: 'Genome') -> 'Genome':\n",
    "    one_cut_point = randint(0, int((LOCI)*0.3))\n",
    "    two_cut_point = randint(int((LOCI)*0.3), int((LOCI)*0.6))\n",
    "    three_cut_point = randint(int((LOCI)*0.6), LOCI - 1)\n",
    "  \n",
    "    # Order the cut points\n",
    "    cut_points = sorted([one_cut_point, two_cut_point, three_cut_point])\n",
    "    \n",
    "    new_ind = Genome()\n",
    "    new_ind.genotype = (ind1.genotype[:cut_points[0]] +\n",
    "                        ind2.genotype[cut_points[0]:cut_points[1]] +\n",
    "                        ind1.genotype[cut_points[1]:cut_points[2]] +\n",
    "                        ind2.genotype[cut_points[2]:])\n",
    "    \n",
    "    new_ind.genotype[4] = min(new_ind.genotype[1], new_ind.genotype[4]) # faccio in modo che max_dim_replay_buff > batch_size\n",
    "    \n",
    "    assert len(new_ind.genotype) == LOCI\n",
    "    return new_ind\n",
    "\n",
    "def compute_fitness(individual: 'Genome'):\n",
    "    TrainedGeneratorNet = TrainedPlayer()\n",
    "    trained_weights = training(individual)\n",
    "    TrainedGeneratorNet.GeneratorNet.load_state_dict(trained_weights) # TRAINED NETWORK to use in INFERENCE PHASE\n",
    "\n",
    "    wins = 0\n",
    "    num_match_test = 100\n",
    "\n",
    "    for step in range(num_match_test):\n",
    "        player=RandomPlayer()\n",
    "        g=Game()\n",
    "        #g.current_player_idx = step%2\n",
    "        g.current_player_idx += 1\n",
    "        winner=g.play(TrainedGeneratorNet, player)\n",
    "        if winner==0:\n",
    "            wins+=1\n",
    "\n",
    "    individual.fitness = (wins/num_match_test)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def genetic_algorithm():\n",
    "        population = [Genome() for _ in range(POPULATION_SIZE)] # starting pouplation of POPULATION_SIZE individuals\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "                compute_fitness(population[i])\n",
    "\n",
    "        population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "        best_fitness = population[0].fitness\n",
    "        best_individual = population[0]\n",
    "        gen = 0\n",
    "\n",
    "        while  gen < NUM_GENERATION:                \n",
    "                for _ in range(OFFSPRING_SIZE):\n",
    "                        offspring = new_offspring(population[0], population[1])\n",
    "                        compute_fitness(offspring)\n",
    "\n",
    "                population.extend([offspring])\n",
    "                population.sort(key=lambda i: i.fitness, reverse=True)\n",
    "                population = population[:POPULATION_SIZE]\n",
    "                best_individual = population[0]\n",
    "                best_fitness = population[0].fitness\n",
    "                gen += 1\n",
    "                print(f\"Best individual (until now): {best_individual.genotype}\")\n",
    "                print(f\"Best fitness: {best_fitness}\")\n",
    "\n",
    "\n",
    "        print(f\"Best individual with fitness: {best_fitness}\")\n",
    "        \n",
    "        return best_individual\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chito\\AppData\\Local\\Temp\\ipykernel_17660\\2825236233.py:86: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  q_values = player1.GeneratorNet(torch.tensor(state_batch, dtype=torch.float32)).to(player1.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 178]\n",
      "Best fitness: 60.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 178]\n",
      "Best fitness: 60.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 178]\n",
      "Best fitness: 60.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 178]\n",
      "Best fitness: 60.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual (until now): [2, 778, 30, 0.4850682937735741, 351]\n",
      "Best fitness: 73.0\n",
      "Best individual with fitness: 73.0\n"
     ]
    }
   ],
   "source": [
    "BEST_INDIVIDUAL = genetic_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Genome' object has no attribute 'loss_tot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m----> 3\u001b[0m vals \u001b[38;5;241m=\u001b[39m [el\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[43mBEST_INDIVIDUAL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_tot\u001b[49m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate a sequence of integers to represent the epoch numbers\u001b[39;00m\n\u001b[0;32m      5\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(BEST_INDIVIDUAL\u001b[38;5;241m.\u001b[39mloss_tot) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Genome' object has no attribute 'loss_tot'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "vals = [el.detach().numpy() for el in BEST_INDIVIDUAL.loss_tot]\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, len(BEST_INDIVIDUAL.loss_tot) + 1)\n",
    "\n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, vals, label='Training Loss')\n",
    "\n",
    "# Add title and axes labels\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Display the plot\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
