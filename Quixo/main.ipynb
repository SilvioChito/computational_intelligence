{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random,numpy\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from game import Game, Move, RandomPlayer, MyPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuixoNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5*5, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 44),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "## Constants definition\n",
    "num_iterations = 50_000\n",
    "num_matches = 1\n",
    "max_dim_replay_buff = 10_000\n",
    "time_to_update = 100\n",
    "gamma = 0.001\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:    \n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def add_experience(self, experience):        \n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(experience)        \n",
    "        else:\n",
    "            self.buffer[self.position] = experience        \n",
    "            self.position = (self.position + 1) % self.buffer_size\n",
    "            \n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        #batch = np.random.choice(self.buffer, batch_size, replace=False)        \n",
    "        #states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        ### OCCHIO QUAAAA !!!!! ###\n",
    "        \n",
    "        return (np.array(states),\n",
    "                np.array(actions),            \n",
    "                np.array(rewards),\n",
    "                np.array(next_states),            \n",
    "                np.array(dones)\n",
    "        )\n",
    "        \"\"\"\n",
    "        return np.random.sample(self.buffer, batch_size)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from game import Player\n",
    "\n",
    "\n",
    "def training(player1: 'Player', player2: 'Player'):\n",
    "    loss_tot = []\n",
    "\n",
    "    replay_buff = ReplayBuffer(max_dim_replay_buff) # replay buffer, from which we sample for BATCH learning\n",
    "    # inizializza i modelli -> passali per parametro\n",
    "    \n",
    "    for step in range(num_iterations):        \n",
    "        # we fill the replay buffer with experiences made with matches\n",
    "        for match in range(num_matches):\n",
    "            # lo uso solo in inferenza\n",
    "            # gioca una intera partita qui dentro e per ogni mossa, metti l'experience dentro il ReplayBuffer\n",
    "            # ...\n",
    "            # ... qui descrivere come viene effettivamente usata la rete neurale (magari impacchettarla in qualche funzione)\n",
    "            winner = -1\n",
    "            g = Game()\n",
    "            #player1 = MyPlayer()\n",
    "            #player2 = RandomPlayer()\n",
    "            players = [player1, player2]\n",
    "            \n",
    "            while winner < 0:\n",
    "                print(\"### miao ###\")\n",
    "                g.current_player_idx += 1\n",
    "                g.current_player_idx %= len(players)\n",
    "                #print(g.current_player_idx)\n",
    "                prev_state=deepcopy(g)\n",
    "                from_pos, slide = players[g.current_player_idx].make_move(g)\n",
    "                   \n",
    "                if g.current_player_idx==0:\n",
    "                    ##l'azione è il valore del q-value quindi un numero? o la posizione in questo caso  \n",
    "                    ##in quel caso replay_row=(prev_state.get_board,GeneratorNet((prev_state.get_board),g.get_board,reward)\n",
    "                    reward=g.compute_reward(from_pos, slide)\n",
    "                    replay_row=(prev_state.get_board, player1.last_action_value, deepcopy(g), reward)\n",
    "                    replay_buff.add_experience(replay_row)\n",
    "                    \n",
    "                if g.check_winner() != -1:\n",
    "                    break\n",
    "            \n",
    "            # replay_buffer.push(...) per ogni transizione di stato osservata ()\n",
    "            # restituisci il risultato sotto forma di tupla (st, a, st+1, r) - (State, action, next_state, reward)\n",
    "            \n",
    "        # Now we sample a batch of data from the ReplayBuffer in order to train the Agent\n",
    "        batch_to_train = replay_buff.sample_batch(30)\n",
    "        TargetNet_targets = []\n",
    "        GeneratorNet_outputs = []\n",
    "        player1.myplayer_zero_grad()\n",
    "        # Now we need to compute the list of targets (made with the TargetNet) and the one with the Q values for the \"current\" state\n",
    "        \n",
    "        for element in batch_to_train:\n",
    "            _, action_val, new_state, reward = element\n",
    "            print(\"### {action_val} ###\")\n",
    "            # per ogni tupla di 4 elementi prendere l'action (a) (sarà un valore...q value)\n",
    "            # Rappresenta il primo termine della Loss function (MSE) => output\n",
    "            # inserirlo dentro GeneratorNet_outputs\n",
    "            GeneratorNet_outputs.append(action_val)\n",
    "\n",
    "            # Per calcolare il secondo termine, devo prendere la reward dalla tupla, gamma dai parametri (vedi sopra) e il q value dell'azione migliore dello stato successivo\n",
    "            # Rappresenta il secondo termine della Loss function (MSE) => target\n",
    "            # Per calcolare il target, dare in pasto alla NN lo stato+1 contenuto nella tupla element\n",
    "            # inserirlo dentro TargetNet_targets\n",
    "            max_action_newstate = player1.compute_target(new_state)\n",
    "            res = reward + gamma*max_action_newstate\n",
    "            TargetNet_targets.append(res)\n",
    "            \n",
    "        loss_curr = player1.myplayer_loss_and_update_params(GeneratorNet_outputs, TargetNet_targets) \n",
    "        loss_tot.append(loss_curr)\n",
    "        \n",
    "        if (step % time_to_update) == 0:\n",
    "            # update the parameter of the TargetNet\n",
    "            player1.copy_params_TargetNet()\n",
    "            \n",
    "        print(loss_curr)\n",
    "        #printa come varia \n",
    "            \n",
    "    return player1.GeneratorNet.state_dict()\n",
    "            \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n",
      "### miao ###\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.float32 object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/silvio/Desktop/CI_project/main.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m player1 \u001b[39m=\u001b[39m MyPlayer()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m player2 \u001b[39m=\u001b[39m RandomPlayer()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trained_model_params \u001b[39m=\u001b[39m training(player1, player2)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m TrainedGeneratorNet \u001b[39m=\u001b[39m QuixoNet\u001b[39m.\u001b[39mload_state_dict(trained_model_params) \u001b[39m# TRAINED NETWORK to use in INFERENCE PHASE\u001b[39;00m\n",
      "\u001b[1;32m/home/silvio/Desktop/CI_project/main.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# Now we need to compute the list of targets (made with the TargetNet) and the one with the Q values for the \"current\" state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m batch_to_train:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     _, action_val, new_state, reward \u001b[39m=\u001b[39m element\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m### \u001b[39m\u001b[39m{action_val}\u001b[39;00m\u001b[39m ###\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# per ogni tupla di 4 elementi prendere l'action (a) (sarà un valore...q value)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m# Rappresenta il primo termine della Loss function (MSE) => output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/Desktop/CI_project/main.ipynb#W6sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m# inserirlo dentro GeneratorNet_outputs\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.float32 object"
     ]
    }
   ],
   "source": [
    "player1 = MyPlayer()\n",
    "player2 = RandomPlayer()\n",
    "trained_model_params = training(player1, player2)\n",
    "TrainedGeneratorNet = QuixoNet.load_state_dict(trained_model_params) # TRAINED NETWORK to use in INFERENCE PHASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = 0\n",
    "losts = 0\n",
    "draws = 0\n",
    "num_match_test = 1000\n",
    "\n",
    "for _ in num_match_test:\n",
    "    # play_match(TrainedGeneratedNet, random)\n",
    "    # increment wins/losts/draws \n",
    "    print(\"miao\")\n",
    "\n",
    "print(f\"Accuracy: {(wins/num_match_test)*100}\")\n",
    "print(\"Wins: {wins} - Losts: {losts} - Draws {draws}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
